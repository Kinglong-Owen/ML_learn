# xgboost

$$
损失函数L(\phi)=\Sigma_il(\hat{y}_i-y_i)+\Sigma_k\Omega(k_k)\\
复杂度\Omega(f)=\gamma T+\frac{1}{2}\lambda ||\omega||^2\\
$$

$l(\hat y_i-y_i)$表示损失函数， $T$表示叶子节点个数，$\omega$表示叶子节点的数值（回归树，分类树中是叶子节点的类别) 

## 重点

- **$\hat y_i$是最优化求出来的**，不是啥平均值或规则指定的，这个算是一个思路上的新颖吧

- 支持并行，在选择最佳分裂的时候并行

## xgboost相对于GBDT的改进

-   **第一，GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶**。保留了更多有关目标函数的信息，对提升效果有帮助。
-  **第二，GBDT是给新的基模型寻找新的拟合标签**（前面加法模型的负梯度），**而xgboost是给新的基模型寻找新的目标函数**（目标函数关于新的基模型的二阶泰勒展开）。
- **第三，xgboost加入了和叶子权重的L2正则化项**，因而有利于模型获得更低的方差。
- **第四，xgboost增加了自动处理缺失值特征的策略。**通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。  
- 此外，xgboost还支持候选分位点切割，特征并行等，可以提升性能。