# 降维算法

### 意义：

- 多余的特征会影响或误导学习器
- 特征太多，参数也多，过拟合风险大
- 数据纬度可能虚高，真实纬度很小
- 纬度越少，训练越快，结果可能更好
- 降维可视化比较方便

### 降维的算法都有哪些

``` mermaid
graph LR
A(降维)-->B(特征抽取)
B-->C(线性降维)
C-->D(主成分分析)
C-->E(因子分析)
C-->F(独立成分分析)
C-->G(线性判别分析)
A-->H(特征筛选)
H-->I(缺失值比较)
I-->CC(超过一定缺失值就给删掉)
H-->J(低方差滤波)
J-->DD(方差太小表示包含的信息少_删除掉)
H-->KI(高相关滤波)
KI-->EE(两个特征的相关性很大_用其中一个来替代另一个)
H-->L(随机森林)
L-->FF(返回重要的特征_不重要的删掉)
H-->M(反向特征消除)
M-->GG(把特征都去训练_踢掉对结果影响最小的特征)
H-->N(前向特征选择)
N-->II(每次只添加最重要的特征)
B-->AA(流形学习_非线性降维)
B-->BB(奇异值分解_SVD)


```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1564925904679.png" width="60%" />   



## PCA 

PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

**计算过程**：寻找数据的协方差矩阵的特征值和特征向量。其中前k大的特征值对应的特征向量就是映射矩阵，可以把当前的数据映射到k维空间，实现数据的降维。本质上是寻找方差最大的轴，映射到这个轴上。

**协方差矩阵** 的（i，j）项是第i个维度和第j个纬度的协方差。

**散度矩阵**：
$$
散度矩阵S=\sum_{k-1}^{n}(x_k-m)(x_k-m)^T \\
m=\frac{1}{n}\sum_{k=1}^{n}x_k
$$
散度矩阵和协方差矩阵的关系
$$
C=\frac{S}{n-1}
$$


## SVD（线性判别分析）

