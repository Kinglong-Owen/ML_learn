# 决策树

## 优缺点

| 优点                                                         | 缺点                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 简单易懂，原理清晰，决策树可以实现可视化                     | 决策树有时候是不稳定的，因为数据微小的变动，可能生成完全不同的决策树 |
| 决策树算法的时间复杂度（预测数据）是用于训练决策树的数据点的对数 | 有些问题学习起来非常难，因为决策树很难表达，如异或问题、奇偶校验或多路复用器问题 |
| 能够处理数值和分类数据                                       | 如果有些因素占据支配地位，决策树是有偏差的。因此建议在拟合决策树之前先平衡数据的影响因子。 |
| 能够处理多路输出的问题                                       | 对连续性字段比较难预测                                       |
| 可以通过统计学检验验证模型。这也使模型的可靠性计算变得可能   | 最优决策树的构建属于NP问题                                   |
| 即使模型假设违反产生数据的真实模型，表现性能依旧良好         |                                                              |

## 奥卡姆剃刀

两个差不多的模型，优先选择简单的模型

## 熵

熵的大小与信息的不确定性有关，不确定性越大，熵越大

信息熵公式$H[x]=-\Sigma_xp(x)log_2p(x)$

信息增益的公式$info(D)=-\Sigma^m_xp_ilog_2(p_i)​$

​			$info_A(D)=\Sigma_{j-1}^v\frac{|D_j|}{|D|}*info(D_j)​$

信息增益$Gain(A)=Info(D)-Info_A(D)$ 

==信息增益越大，表示新属性带给旧属性的不确定的减少量越大。通俗的说就是新属性带来了更多的确定信息，故越大的属性越靠近根节点==

![1565410441040](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565410441040.png) 

## 算法分三种：ID3，C4.5和CART

主要的区别在于特征选择依据的不同

### ID3算法

**ID3算法的核心**是在决策树各个节点上对应信息增益准则选择特征，递归的构建决策树。倾向于选择分支比较多的属性。==不能处理连续性特征== 

**具体方法**：

- 从根节点开始，对每个计算所有可能的特征的信息增益
- 选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点
- 再对子节点调用以上方法，构建决策树
- 直到多有特征的特征增益均很小或者没有特征可以选择为止

**递归算法结束的条件**：程序遍历完所有特征列，或者每个分支下的所有实例都属于相同的分类

所有实例具有相同分类，则得到一个叶子节点。任何到达叶节点的必须是属于叶节点的分类，即叶节点里面必须是标签。

**剪枝** 可以通过裁减合并相邻的无法产生大量信息增益的叶子节点（如设置信息增益阈值）

### C4.5算法

==能支持连续性特征== 

**增益率的分子** 就是ID3算法的增益$Gain(A)$ 

**增益率的分母** $SpltInfo_A(D)=-\Sigma_{i=1}^v\frac{|D_j|}{|D|}*log_2(\frac{|D_j|}{|D|})$  ,

**增益率**              $Gainrate(A)=\frac{Gain(A)}{SplitInfo_A(D)}$ 

### CART算法

详见CART.md

## 对比

ID3和C4.5生成的树不一定是二叉树，而CART一定是二叉树

ID3和C4.5的寻找最佳分裂的计算过程伪代码（包含标签量和连续量）

```
#离散值
def 最佳分裂的函数(dataset):
	info_dataset=dataset的香农熵
	循环所有特征字段:
		如果这个字段是离散型数据：#找到最佳分裂的特征后，左右子树不包含本特征
			循环每个特征的所有类别：
				计算每个类别的香农熵
				info_a_d=累加这个特征字段的每个类别占新属性的比例*每个类别的香农熵
			信息增益=info_dataset-info_a_d
			寻找信息增益最大的特征
		如果这个字段是连续型数据：#找到最佳分裂的中值后，左右子树依旧包含本特征
			set(本特征的数据)
			排序set
			计算相邻的数据的中值，总个数比原来数据少一个
			for item in 这些中值：
				len1=len(dataset[dataset[本特征]<item])
				len2=len(dataset[dataset[本特征]>=item])
				香农熵1=计算dataset[dataset[本特征]<item]的香农熵
				香农熵2=计算dataset[dataset[本特征]>=item]的香农熵
				信息增益=info_dataset-len1/本特征数据总长度*香农熵1+len2/本特征数据总长度*香农熵2
				寻找信息增益最大的特征
```

CART-寻找最佳分裂的计算过程伪代码

```
def 计算基尼系数(dataset):
	pass
def 最佳分裂的函数(dataset):
	
```

