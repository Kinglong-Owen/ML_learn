# CART算法

可用于**分类和回归**，采用Gini指数（选Gini指数最小的特征）作为分裂标准，同时也包含了后续的剪枝操作。  

构建一个决策树的四个问题：

1. 特征向量有多个分量，每个决策节点上应该选择哪个分量

2. 选定一个特征之后，判定的规则是什么？

   对于分类问题，要保证分裂后的左右子树样本尽可能的纯，用不纯度衡量。不纯度包括熵不纯度（即香农熵）、Gini不纯度、误分类不纯度等

   **Gini不纯度**：
   $$
   G(D)=1-\Sigma_ip_i^2=1-\frac{\Sigma_iN_i^2}{N^2}
   $$
   当样本属于某一类时Gini不纯度的值最小，最小值为0；当样本均匀地分布于每一类时，Gini不纯度最大。Ni表示第i类样本的个数，N表示总样本数。

   误分类不纯度： 
   $$
   E(D)=1-max(p_i)
   $$
   当样本属于某一类时，不纯度最小；样本均匀时，不纯度最大

## CART分类树

**分裂的总的不纯度**：
$$
G=\frac{N_{LEFT}}{N}G(D_{LEFT})+\frac{N_{RIGHT}}{N}G(D_{RIGHT})
$$
其中，$G(D_{LEFT})$ 是左子集的不纯度，$N_{LEFT}$ 是左子集的样本数，N是总样本数
$$
代入不纯度定义 G=1-\frac{1}{N}(\frac{\Sigma_iN_{L,i}^2}{N_L}+\frac{\Sigma_iN_{R,i}^2}{N_R})
$$

$$
最小化
G=\frac{\Sigma_iN_{L,i}^2}{N_L}+\frac{\Sigma_iN_{R,i}^2}{N_R}
$$

此时的G可以看做是Gini纯度，值越大，越纯

代码中是不化简的：



问题1和问题2合起来即寻找最佳分裂：

3. 何时停止分裂

4. 如何给每个叶节点赋予类别标签

   将叶子节点的值设置成本节点的训练样本集中出现概率最大的那个类

5. 需要注意的是：特征是连续型数据时，依旧用二分法计算基尼系数。处理连续型特征时，划分左右子树之后，最佳分裂的特征应该保留。而处理离散型数据时，划分左右子树之后，应该把最佳分裂的特征删除。

## CART回归树

回归问题：已知数据是一堆自变量和因变量，要求是通过这些已知数据构造自变量和因变量的映射，因变量是连续值

构建一个决策树的四个问题：

1. 特征向量有多个分量，每个决策节点上应该选择哪个分量？

   衡量回归树分裂标准的是回归误差（即样本方差），每次分裂时选用使得方差最小化的那个分裂。
   $$
   样本集的回归误差 E(D)=\frac{1}{l}(\Sigma_{i=1}^ly^2_i-\frac{1}{l}(\Sigma_{j=1}^ly_j)^2)
   $$
   **计算分裂回归误差，最大化最后的公式的分裂就是最佳分裂**
   $$
   分裂的回归误差=分裂之前的回归误差 - \frac{左子树个数}{样本数}左子树的回归误差-\frac{右子树个数}{样本数}右子树的回归误差\\
   分裂的回归误差 E=E(D)-\frac{N_L}{N}E(D_L)-\frac{N_R}{N}E(D_R)            **最大化**\\
   E=\frac{1}{N_L}(\Sigma_{i-1}^{N_L}y_i)^2+\frac{1}{N_R}(\Sigma_{i-1}^{N_R}y_i)^2     **最大化**\\
   $$

2. 选定一个特征之后，判定的规则是什么？

3. 何时停止分裂

   

4. 如何给每个叶节点赋予类别标签

   将叶子节点的值设置成样本**标签值**的均值

5. 处理连续型数据和离散型数据

## 属性缺失问题

1. 直接剔除
2. 寻找替代分裂规则

## 剪枝

### 预剪枝

预剪枝就是在训练过程中剪枝

- 控制树的高度-
- 节点的训练样本数
- 分裂带来的纯度最小提升的最小值 这一点在回归树中用到

### 后剪枝

后剪枝就是训练好一个棵树后再进行剪枝

- 降低错误剪枝(Reduced-Error Pruning,REP)
- 悲观错误剪枝(Pesimistic-Error Pruning,PEP)
- 代价-复杂度剪枝(Cost-Complexity Pruning,CCP)

## 对比

|                | 分类树         | 回归树         |
| -------------- | -------------- | -------------- |
| 分裂标准       | 总的Gini纯度   | 总的误差       |
| 适用的变量类型 | 标签量或连续量 | 标签量或连续量 |
|                |                |                |

<https://blog.csdn.net/gzj_1101/article/details/78355234#%E5%89%AA%E6%9E%9D%E5%A4%84%E7%90%86>

<https://blog.csdn.net/enjoy524/article/details/54910978>

<https://blog.csdn.net/LY_ysys629/article/details/72809129>	

<https://blog.csdn.net/baimafujinji/article/details/53269040#commentBox>

https://blog.csdn.net/wzmsltw/article/details/51057311